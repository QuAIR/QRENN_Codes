{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import quairkit as qkit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from quairkit.database.state import zero_state, ghz_state\n",
    "from quairkit.database.matrix import *\n",
    "from quairkit.database import haar_orthogonal, haar_unitary, random_unitary_hermitian, pauli_group, random_clifford\n",
    "from quairkit.core.state import to_state\n",
    "from quairkit.circuit import Circuit\n",
    "from quairkit.qinfo import permute_systems, partial_transpose, trace_distance, dagger, NKron, trace\n",
    "\n",
    "qkit.set_dtype('complex128')\n",
    "NUMPY_DTYPE = 'complex128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrenn_cir(data_generators:torch.Tensor,\n",
    "             trainable_qubits:int, \n",
    "             V_number_qubits:int, \n",
    "             layers:int, \n",
    "             is_conj:bool = False) -> Circuit:\n",
    "    \n",
    "    cir = Circuit(trainable_qubits + V_number_qubits)\n",
    "    \n",
    "    cir.h(qubits_idx=list(range(trainable_qubits, trainable_qubits+V_number_qubits)))\n",
    "    for _ in range(layers):\n",
    "        cir.universal_qudits(qubits_idx=list(range(trainable_qubits)))\n",
    "        if is_conj:\n",
    "            if _ % 2 == 0:\n",
    "                cir.control_oracle(data_generators, system_idx = [list(range(trainable_qubits))] + \n",
    "                                    list(range(trainable_qubits, V_number_qubits +trainable_qubits)), gate_name='g(U)')\n",
    "            else:\n",
    "                cir.control_oracle(dagger(data_generators), system_idx = [list(range(trainable_qubits))] + \n",
    "                                    list(range(trainable_qubits, V_number_qubits +trainable_qubits)), gate_name='g(U^{\\dagger})')\n",
    "        else:\n",
    "            cir.control_oracle(data_generators, system_idx = [list(range(trainable_qubits))] + \n",
    "                                    list(range(trainable_qubits, V_number_qubits +trainable_qubits)), gate_name='g(U)')\n",
    "    # cir.u3(qubits_idx=list(range(trainable_qubits)))\n",
    "    cir.universal_qudits(qubits_idx=list(range(trainable_qubits)))\n",
    "\n",
    "    return cir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(v:qkit.State, num_labels:int, m:torch.Tensor):\n",
    "    return torch.norm(trace(m @ v.density_matrix).squeeze() - num_labels, p=2) / len(num_labels)\n",
    "\n",
    "def empirical_loss(v:qkit.State, num_labels:int, m:torch.Tensor):\n",
    "    return torch.mean( - num_labels * torch.abs(trace(m @ v.density_matrix)))\n",
    "\n",
    "def proj_loss(v:qkit.State, num_labels:int):\n",
    "    P00 = (1.0+0.0j) * torch.tensor([[1,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]])\n",
    "    P11 = (1.0+0.0j) * torch.tensor([[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,1]])\n",
    "    projplus = P00 + P11\n",
    "    projminus = torch.eye(4) - projplus\n",
    "    rho = v.trace(list(range(2,v.num_qubits)))\n",
    "    rho = rho.density_matrix\n",
    "    loss = 0.0\n",
    "    for j in range(len(num_labels)):\n",
    "        if num_labels[j] == -1:\n",
    "            loss += torch.trace(rho[j] @ projminus)\n",
    "        else:\n",
    "            loss += torch.trace(rho[j] @ projplus)\n",
    "    return -torch.real(loss) / len(num_labels)\n",
    "    \n",
    "\n",
    "# Training\n",
    "def train_model_convergence(u:torch.Tensor,\n",
    "                y_labels:torch.Tensor,\n",
    "                trainable_qubits:int,\n",
    "                input_state:qkit.State, \n",
    "                ITR:int = 100, \n",
    "                LR:float = 0.1, \n",
    "                slot:int = 10):\n",
    "    \n",
    "    assert u.shape[0] == y_labels.shape[0], \"Number of Hams and y_labels should be the same\"\n",
    "    \n",
    "    dim_h = u[0].shape[0]\n",
    "    V_number_qubits = int(np.log2(dim_h))\n",
    "    \n",
    "    loss_list, time_list = [], []\n",
    "    \n",
    "    # initialize the model\n",
    "    cir = qrenn_cir(u, trainable_qubits, V_number_qubits, layers=slot)\n",
    "    \n",
    "    # cir is a Circuit type\n",
    "    opt = torch.optim.Adam(lr=LR, params=cir.parameters())\n",
    "\n",
    "    # activate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, \"min\", factor=0.5)\n",
    "\n",
    "    print(\"Training:\")\n",
    "    _ = 0\n",
    "    while _ < ITR:\n",
    "\n",
    "        start_time = time.time()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss = proj_loss(cir(input_state), y_labels)\n",
    "\n",
    "        loss.backward()  # compute gradients\n",
    "        opt.step()  # update parameters\n",
    "        scheduler.step(loss)  # activate scheduler\n",
    "\n",
    "        loss = loss.item()\n",
    "        loss_list.append(loss)\n",
    "        time_list.append(time.time() - start_time)\n",
    "\n",
    "        if scheduler.get_last_lr()[0]<2e-8:\n",
    "            print(\n",
    "                f\"iter: {_}, loss: {loss:.8f}, lr: {scheduler.get_last_lr()[0]:.2E}, avg_time: {np.mean(time_list):.4f}s\"\n",
    "            )\n",
    "            time_list = []\n",
    "            break\n",
    "\n",
    "        if _ % 100 == 0 or _ == ITR - 1:\n",
    "            # print(\n",
    "            #     f\"iter: {_}, loss: {loss:.8f}, lr: {scheduler.get_last_lr()[0]:.2E}, avg_time: {np.mean(time_list):.4f}s\"\n",
    "            # )\n",
    "            print(\n",
    "                f\"iter: {_}, loss: {loss:.8f}, lr: {scheduler.get_last_lr()[0]:.2E}, avg_time: {np.mean(time_list):.4f}s\"\n",
    "            )\n",
    "            time_list = []\n",
    "        _ += 1\n",
    "    return cir, loss_list\n",
    "\n",
    "\n",
    "def predict_model(u:torch.Tensor,\n",
    "                  trainable_qubits:int, \n",
    "                  params:torch.Tensor,\n",
    "                  input_state:qkit.State, \n",
    "                  slot:int,\n",
    "                  scale:float=1.0):\n",
    "    \n",
    "    z0 = qkit.Hamiltonian([[scale, \",\".join([f\"Z{i}\" for i in range(trainable_qubits)])]])\n",
    "    \n",
    "    dim_h = u[0].shape[0]\n",
    "    V_number_qubits = int(np.log2(dim_h))\n",
    "\n",
    "    # initialize the model\n",
    "    cir = qrenn_cir(u, trainable_qubits, V_number_qubits, layers=slot)\n",
    "    cir.update_param(params)\n",
    "\n",
    "    return cir(input_state).expec_val(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "def uhermitian_data_generation(dim:int, num_samples:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    sour = np.random.randint(0, 2, num_samples)\n",
    "    y_labels = np.where(sour == 0, -1, 1)\n",
    "    data_U = np.zeros((num_samples, dim, dim), dtype=NUMPY_DTYPE)\n",
    "    for y in range(num_samples):\n",
    "        if sour[y]:\n",
    "             # Randomly choose eigenvalues of +1 or -1\n",
    "            eigenvalues = np.random.choice([1.0, -1.0], size=dim)\n",
    "            D = np.diag(eigenvalues)\n",
    "            \n",
    "            # Generate a random unitary matrix U\n",
    "            U = haar_unitary(dim)\n",
    "            data_U[y, :, :] = U @ D @ U.conj().T\n",
    "\n",
    "        else:\n",
    "            data_U[y, :, :] = haar_unitary(dim)\n",
    "    \n",
    "    return torch.tensor(y_labels), torch.tensor(data_U)\n",
    "\n",
    "\n",
    "def uhermitian_data_generation_erd(num_samples:int, V_number_qubits) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    n = 2**V_number_qubits\n",
    "    sour = np.random.randint(0, 2, num_samples)\n",
    "    y_labels = torch.tensor(np.where(sour == 0, -1, 1))\n",
    "    data_U = torch.zeros((num_samples, n, n), dtype=torch.complex128)\n",
    "    for y in range(num_samples):\n",
    "        if sour[y]:\n",
    "            eigenvalues = (torch.randint(0, 2, (n,)) * 2 -1).to(torch.complex128)\n",
    "            D = torch.diag(eigenvalues)\n",
    "            U = haar_unitary(n)\n",
    "            \n",
    "            # Generate a random unitary matrix U\n",
    "            data_U[y, :, :] = U @ D @ U.conj().T\n",
    "        else:\n",
    "            \n",
    "            mat = np.random.randn(n, n) + 1j * np.random.randn(n, n)\n",
    "            for i in range(n):\n",
    "                mat[i, i] = np.abs(mat[i, i])\n",
    "                for j in range(i):\n",
    "                    mat[i, j] = np.conj(mat[j, i])\n",
    "                    \n",
    "            data_U[y, :, :] = torch.matrix_exp(-1j * torch.tensor(mat, dtype=torch.complex128))\n",
    "                \n",
    "    return y_labels, data_U\n",
    "\n",
    "\n",
    "def random_diag_data_generation(dim:int, num_samples:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    sour = np.random.randint(0, 2, num_samples)\n",
    "    y_labels = np.where(sour == 0, -1, 1)\n",
    "    data_U = np.zeros((num_samples, dim, dim), dtype=NUMPY_DTYPE)\n",
    "    for y in range(num_samples):\n",
    "        if sour[y]:\n",
    "             # Randomly choose eigenvalues of +1 or -1\n",
    "            eigenphase = 1*np.pi*np.random.random(size=dim)\n",
    "            data_U[y, :, :] = np.diag(np.exp(1j*eigenphase))\n",
    "        else:\n",
    "            data_U[y, :, :] = haar_unitary(dim)\n",
    "    \n",
    "    return torch.tensor(y_labels), torch.tensor(data_U)\n",
    "\n",
    "\n",
    "def random_pauli_data_generation(dim:int, num_samples:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    sour = np.random.randint(0, 2, num_samples)\n",
    "    n = int(np.log2(dim))\n",
    "    y_labels = np.where(sour == 0, -1, 1)\n",
    "    data_U = np.zeros((num_samples, dim, dim), dtype=NUMPY_DTYPE)\n",
    "    for y in range(num_samples):\n",
    "        if sour[y]:\n",
    "            data_U[y, :, :] = pauli_group(n)[np.random.randint(1, 4**n)]\n",
    "        else:\n",
    "            data_U[y, :, :] = haar_unitary(dim)\n",
    "    \n",
    "    return torch.tensor(y_labels), torch.tensor(data_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_qubits = 2\n",
    "V_number_qubits = 6\n",
    "\n",
    "# data generation\n",
    "train_samples = 100\n",
    "test_samples = 500\n",
    "\n",
    "# labels, data_raw = random_pauli_data_generation(2**V_number_qubits, train_samples+test_samples)\n",
    "labels, data_raw = uhermitian_data_generation(2**V_number_qubits, train_samples+test_samples)\n",
    "# labels, data_raw = uhermitian_data_generation_erd(train_samples+test_samples, V_number_qubits)\n",
    "# labels, data_raw = random_diag_data_generation(2**V_number_qubits, train_samples+test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "iter: 0, loss: -0.56192090, lr: 1.00E-01, avg_time: 0.1652s\n",
      "iter: 100, loss: -0.73371173, lr: 1.00E-01, avg_time: 0.1708s\n",
      "iter: 200, loss: -0.73627283, lr: 1.00E-01, avg_time: 0.1714s\n",
      "iter: 300, loss: -0.73671915, lr: 1.00E-01, avg_time: 0.1727s\n",
      "iter: 400, loss: -0.73698366, lr: 1.00E-01, avg_time: 0.1777s\n",
      "iter: 500, loss: -0.72947962, lr: 1.00E-01, avg_time: 0.1781s\n",
      "iter: 600, loss: -0.73701254, lr: 1.00E-01, avg_time: 0.1751s\n",
      "iter: 700, loss: -0.73696873, lr: 5.00E-02, avg_time: 0.1782s\n",
      "iter: 800, loss: -0.73701765, lr: 5.00E-02, avg_time: 0.1750s\n",
      "iter: 900, loss: -0.73701861, lr: 5.00E-02, avg_time: 0.1737s\n",
      "iter: 1000, loss: -0.73701921, lr: 5.00E-02, avg_time: 0.1754s\n",
      "iter: 1100, loss: -0.73701960, lr: 5.00E-02, avg_time: 0.1742s\n",
      "iter: 1200, loss: -0.73701987, lr: 5.00E-02, avg_time: 0.1774s\n",
      "iter: 1300, loss: -0.73702006, lr: 5.00E-02, avg_time: 0.1757s\n",
      "iter: 1399, loss: -0.73702019, lr: 5.00E-02, avg_time: 0.1747s\n"
     ]
    }
   ],
   "source": [
    "slot = 8\n",
    "LR = 0.1\n",
    "ITR = 1400\n",
    "noise_rate = 0.05\n",
    "input_state = zero_state(trainable_qubits+V_number_qubits)\n",
    "\n",
    "# Create a shuffled copy of psi states\n",
    "shuffled_indices = np.random.permutation(labels.shape[0])\n",
    "data_raw = data_raw[shuffled_indices]\n",
    "labels = labels[shuffled_indices]\n",
    "\n",
    "y_train, x_train = labels[:train_samples], data_raw[:train_samples]\n",
    "y_test, x_test = labels[train_samples:], data_raw[train_samples:]\n",
    "\n",
    "for j in range(train_samples):\n",
    "        if np.random.random() < noise_rate:\n",
    "            y_train[j] = -y_train[j]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "cir, loss_ = train_model_convergence(u=x_train,\n",
    "                              y_labels=y_train,\n",
    "                              trainable_qubits=trainable_qubits, \n",
    "                              input_state=input_state,\n",
    "                              ITR = ITR, \n",
    "                              LR = LR, \n",
    "                              slot = slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cir.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  2  2  0  2  0  0  0  0  0 -2  0  2  0  0  0  2  0  0  2  0\n",
      "  0 -2  2  0  0  0 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  2  0  0  0  0  0  0  0 -2  0 -2  0  0  0  0  0 -2  0  0  0  0  0\n",
      "  0  0  2 -2  0  0  0  0  0  0  0  0  0 -2 -2  0  2  0 -2  0  0  0  0  0\n",
      " -2  0  0  0]\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "h_train = NKron(*[z() for _ in range(trainable_qubits)])\n",
    "h_data = NKron(*[torch.eye(2, dtype=torch.complex128) for _ in range(V_number_qubits)])\n",
    "m = NKron(h_train, h_data)\n",
    "\n",
    "v = cir(input_state)\n",
    "diff = np.where(trace(m @ v.density_matrix).detach().squeeze().numpy().real >= 0.0, 1, -1) - y_train.numpy()\n",
    "print(diff)\n",
    "print(np.sum(diff == 0) / diff.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  2  0  0  0  0  2  0 -2  0  0 -2  0  0  0  0  0 -2  0  2  0  0  0\n",
      "  0  0  0  2  0  0  0  0  0  0  2  0  0  0  0  2  0  0  0  2  0  0  0  0\n",
      "  0  0 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -2  0\n",
      "  0  2  0  0  0  0  0  0  0  2  0  0 -2  0  0  0  0  0  0  0  0  0  0 -2\n",
      "  0  2  0  2  0  0 -2  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -2  0  0 -2  0  0  2  0  0  0\n",
      "  2  0  0  0 -2  0  0 -2  0  0  0  0  0  0 -2  0  0  0  0  0  0 -2  0 -2\n",
      "  0  0 -2  0  0  0  0  0  0  0  0  0  2  0  0  0  0 -2  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 -2  2  0  0  0  0  0  0  0 -2  0  0  2  0  0  0\n",
      "  0  0 -2  0 -2  0  0  0 -2  0  0  2  0  0  0 -2  0  0 -2  0  2  0  0 -2\n",
      "  2  0  0  0  0  0  2  0  2  0  0  0  0  0  0  0  0  2  0  0 -2  0 -2  0\n",
      "  2  0  0  2  0  0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 -2  0  0  0  0  0  0 -2 -2  0  0 -2 -2  0  0  0  2\n",
      "  0  0  0  0  0  2  0  0  0  0  0 -2  0  2  0  0 -2  0  0  0  0  0  0  2\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0 -2  0  0\n",
      " -2  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  2  0 -2  0  0  0 -2  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0 -2  0  0  0  0  0  0\n",
      " -2  0  0  2  2  0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0  0]\n",
      "0.834\n"
     ]
    }
   ],
   "source": [
    "y_predict = predict_model(x_test,\n",
    "                            trainable_qubits=trainable_qubits, \n",
    "                            params=cir.param, \n",
    "                            input_state=input_state,\n",
    "                            slot=slot)\n",
    "diff = np.where(y_predict.detach().numpy() >= 0, 1, -1) - y_test.numpy()\n",
    "print(diff)\n",
    "print(np.sum(diff == 0) / diff.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
